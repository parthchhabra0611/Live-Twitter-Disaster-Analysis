{"cells":[{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install transformers","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting transformers\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n\u001b[K    100% |████████████████████████████████| 778kB 6.0MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers) (1.16.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2019.4.14)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.0.4)\nCollecting sacremoses (from transformers)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n\u001b[K    100% |████████████████████████████████| 890kB 4.1MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.1.82)\nCollecting tokenizers==0.8.1.rc1 (from transformers)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n\u001b[K    100% |████████████████████████████████| 3.0MB 4.2MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.21.0)\nRequirement already satisfied: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers) (0.6)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.31.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers) (17.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.3.9)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.6)\nRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.22)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers) (2.2.0)\nBuilding wheels for collected packages: sacremoses\n  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\nSuccessfully built sacremoses\nInstalling collected packages: sacremoses, tokenizers, transformers\nSuccessfully installed sacremoses-0.0.43 tokenizers-0.8.1rc1 transformers-3.0.2\n\u001b[33mYou are using pip version 19.0.3, however version 20.2.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install clean-text[gpl]","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting clean-text[gpl]\n  Downloading https://files.pythonhosted.org/packages/e3/a4/cb7b851f1f7ae68a128482cd57ff0c4c96b64083f41ca5e9608e2a2889a5/clean_text-0.2.1-py3-none-any.whl\nCollecting ftfy<6.0,>=5.8 (from clean-text[gpl])\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n\u001b[K    100% |████████████████████████████████| 71kB 651kB/s ta 0:00:011\n\u001b[?25hCollecting unidecode<2.0.0,>=1.1.1; extra == \"gpl\" (from clean-text[gpl])\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n\u001b[K    100% |████████████████████████████████| 245kB 1.8MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy<6.0,>=5.8->clean-text[gpl]) (0.1.7)\nBuilding wheels for collected packages: ftfy\n  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\nSuccessfully built ftfy\n\u001b[31mtextacy 0.6.2 has requirement ftfy<5.0.0,>=4.2.0, but you'll have ftfy 5.8 which is incompatible.\u001b[0m\nInstalling collected packages: ftfy, unidecode, clean-text\n  Found existing installation: ftfy 4.4.3\n    Uninstalling ftfy-4.4.3:\n      Successfully uninstalled ftfy-4.4.3\n  Found existing installation: Unidecode 1.0.23\n    Uninstalling Unidecode-1.0.23:\n      Successfully uninstalled Unidecode-1.0.23\nSuccessfully installed clean-text-0.2.1 ftfy-5.8 unidecode-1.1.1\n\u001b[33mYou are using pip version 19.0.3, however version 20.2.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport random\nimport keras\nimport tensorflow as tf\nimport json\nsys.path.insert(0, '../input/pretrained-bert-including-scripts/master/bert-master')\n!cp -r '../input/kerasbert/keras_bert' '/kaggle/working'\nBERT_PRETRAINED_DIR = '../input/bert-base-folder'\nprint('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))","execution_count":3,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"***** BERT pretrained directory: ../input/bert-base-folder *****\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Load raw model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert.keras_bert.bert import get_model\nfrom keras_bert.keras_bert.loader import load_trained_model_from_checkpoint\nfrom keras.optimizers import Adam\nadam = Adam(lr=2e-5,decay=0.01)\nmaxlen = 50\nprint('begin_build')\n\nconfig_file = os.path.join('../input/bert-base-folder/bert_config.json')\ncheckpoint_file = os.path.join('../input/bert-base-folder/bert_model.ckpt')\nmodel = load_trained_model_from_checkpoint(config_file, checkpoint_file, training=True,seq_len=maxlen)\nmodel.summary(line_length=120)","execution_count":4,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nbegin_build\nWARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n________________________________________________________________________________________________________________________\nLayer (type)                           Output Shape               Param #       Connected to                            \n========================================================================================================================\nInput-Token (InputLayer)               (None, 50)                 0                                                     \n________________________________________________________________________________________________________________________\nInput-Segment (InputLayer)             (None, 50)                 0                                                     \n________________________________________________________________________________________________________________________\nEmbedding-Token (TokenEmbedding)       [(None, 50, 768), (30522,  23440896      Input-Token[0][0]                       \n________________________________________________________________________________________________________________________\nEmbedding-Segment (Embedding)          (None, 50, 768)            1536          Input-Segment[0][0]                     \n________________________________________________________________________________________________________________________\nEmbedding-Token-Segment (Add)          (None, 50, 768)            0             Embedding-Token[0][0]                   \n                                                                                Embedding-Segment[0][0]                 \n________________________________________________________________________________________________________________________\nEmbedding-Position (PositionEmbedding) (None, 50, 768)            38400         Embedding-Token-Segment[0][0]           \n________________________________________________________________________________________________________________________\nEmbedding-Dropout (Dropout)            (None, 50, 768)            0             Embedding-Position[0][0]                \n________________________________________________________________________________________________________________________\nEmbedding-Norm (LayerNormalization)    (None, 50, 768)            1536          Embedding-Dropout[0][0]                 \n________________________________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Embedding-Norm[0][0]                    \n________________________________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-1-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Embedding-Norm[0][0]                    \n                                                                                Encoder-1-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-1-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-1-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-1-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-1-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-1-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-1-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-1-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-1-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-1-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-1-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-1-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-2-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-1-FeedForward-Norm[0][0]        \n                                                                                Encoder-2-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-2-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-2-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-2-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-2-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-2-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-2-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-2-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-2-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-2-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-2-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-2-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-3-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-2-FeedForward-Norm[0][0]        \n                                                                                Encoder-3-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-3-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-3-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-3-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-3-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-3-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-3-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-3-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-3-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-3-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-3-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-3-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-4-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-3-FeedForward-Norm[0][0]        \n                                                                                Encoder-4-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-4-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-4-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-4-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-4-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-4-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-4-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-4-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-4-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-4-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-4-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-4-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-5-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-4-FeedForward-Norm[0][0]        \n                                                                                Encoder-5-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-5-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-5-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-5-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-5-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-5-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-5-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-5-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-5-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-5-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-5-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-5-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-6-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-5-FeedForward-Norm[0][0]        \n                                                                                Encoder-6-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-6-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-6-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-6-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-6-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-6-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-6-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-6-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-6-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-6-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-6-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-6-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-7-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-6-FeedForward-Norm[0][0]        \n                                                                                Encoder-7-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-7-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-7-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-7-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-7-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-7-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-7-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-7-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-7-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-7-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-7-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-7-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-8-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-7-FeedForward-Norm[0][0]        \n                                                                                Encoder-8-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-8-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-8-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-8-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-8-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-8-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-8-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-8-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-8-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-8-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-8-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttention (Mult (None, 50, 768)            2362368       Encoder-8-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttention-Dropo (None, 50, 768)            0             Encoder-9-MultiHeadSelfAttention[0][0]  \n________________________________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttention-Add ( (None, 50, 768)            0             Encoder-8-FeedForward-Norm[0][0]        \n                                                                                Encoder-9-MultiHeadSelfAttention-Dropout\n________________________________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttention-Norm  (None, 50, 768)            1536          Encoder-9-MultiHeadSelfAttention-Add[0][\n________________________________________________________________________________________________________________________\nEncoder-9-FeedForward (FeedForward)    (None, 50, 768)            4722432       Encoder-9-MultiHeadSelfAttention-Norm[0]\n________________________________________________________________________________________________________________________\nEncoder-9-FeedForward-Dropout (Dropout (None, 50, 768)            0             Encoder-9-FeedForward[0][0]             \n________________________________________________________________________________________________________________________\nEncoder-9-FeedForward-Add (Add)        (None, 50, 768)            0             Encoder-9-MultiHeadSelfAttention-Norm[0]\n                                                                                Encoder-9-FeedForward-Dropout[0][0]     \n________________________________________________________________________________________________________________________\nEncoder-9-FeedForward-Norm (LayerNorma (None, 50, 768)            1536          Encoder-9-FeedForward-Add[0][0]         \n________________________________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttention (Mul (None, 50, 768)            2362368       Encoder-9-FeedForward-Norm[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttention-Drop (None, 50, 768)            0             Encoder-10-MultiHeadSelfAttention[0][0] \n________________________________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttention-Add  (None, 50, 768)            0             Encoder-9-FeedForward-Norm[0][0]        \n                                                                                Encoder-10-MultiHeadSelfAttention-Dropou\n________________________________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttention-Norm (None, 50, 768)            1536          Encoder-10-MultiHeadSelfAttention-Add[0]\n________________________________________________________________________________________________________________________\nEncoder-10-FeedForward (FeedForward)   (None, 50, 768)            4722432       Encoder-10-MultiHeadSelfAttention-Norm[0\n________________________________________________________________________________________________________________________\nEncoder-10-FeedForward-Dropout (Dropou (None, 50, 768)            0             Encoder-10-FeedForward[0][0]            \n________________________________________________________________________________________________________________________\nEncoder-10-FeedForward-Add (Add)       (None, 50, 768)            0             Encoder-10-MultiHeadSelfAttention-Norm[0\n                                                                                Encoder-10-FeedForward-Dropout[0][0]    \n________________________________________________________________________________________________________________________\nEncoder-10-FeedForward-Norm (LayerNorm (None, 50, 768)            1536          Encoder-10-FeedForward-Add[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttention (Mul (None, 50, 768)            2362368       Encoder-10-FeedForward-Norm[0][0]       \n________________________________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttention-Drop (None, 50, 768)            0             Encoder-11-MultiHeadSelfAttention[0][0] \n________________________________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttention-Add  (None, 50, 768)            0             Encoder-10-FeedForward-Norm[0][0]       \n                                                                                Encoder-11-MultiHeadSelfAttention-Dropou\n________________________________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttention-Norm (None, 50, 768)            1536          Encoder-11-MultiHeadSelfAttention-Add[0]\n________________________________________________________________________________________________________________________\nEncoder-11-FeedForward (FeedForward)   (None, 50, 768)            4722432       Encoder-11-MultiHeadSelfAttention-Norm[0\n________________________________________________________________________________________________________________________\nEncoder-11-FeedForward-Dropout (Dropou (None, 50, 768)            0             Encoder-11-FeedForward[0][0]            \n________________________________________________________________________________________________________________________\nEncoder-11-FeedForward-Add (Add)       (None, 50, 768)            0             Encoder-11-MultiHeadSelfAttention-Norm[0\n                                                                                Encoder-11-FeedForward-Dropout[0][0]    \n________________________________________________________________________________________________________________________\nEncoder-11-FeedForward-Norm (LayerNorm (None, 50, 768)            1536          Encoder-11-FeedForward-Add[0][0]        \n________________________________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttention (Mul (None, 50, 768)            2362368       Encoder-11-FeedForward-Norm[0][0]       \n________________________________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttention-Drop (None, 50, 768)            0             Encoder-12-MultiHeadSelfAttention[0][0] \n________________________________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttention-Add  (None, 50, 768)            0             Encoder-11-FeedForward-Norm[0][0]       \n                                                                                Encoder-12-MultiHeadSelfAttention-Dropou\n________________________________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttention-Norm (None, 50, 768)            1536          Encoder-12-MultiHeadSelfAttention-Add[0]\n________________________________________________________________________________________________________________________\nEncoder-12-FeedForward (FeedForward)   (None, 50, 768)            4722432       Encoder-12-MultiHeadSelfAttention-Norm[0\n________________________________________________________________________________________________________________________\nEncoder-12-FeedForward-Dropout (Dropou (None, 50, 768)            0             Encoder-12-FeedForward[0][0]            \n________________________________________________________________________________________________________________________\nEncoder-12-FeedForward-Add (Add)       (None, 50, 768)            0             Encoder-12-MultiHeadSelfAttention-Norm[0\n                                                                                Encoder-12-FeedForward-Dropout[0][0]    \n________________________________________________________________________________________________________________________\nEncoder-12-FeedForward-Norm (LayerNorm (None, 50, 768)            1536          Encoder-12-FeedForward-Add[0][0]        \n________________________________________________________________________________________________________________________\nMLM-Dense (Dense)                      (None, 50, 768)            590592        Encoder-12-FeedForward-Norm[0][0]       \n________________________________________________________________________________________________________________________\nMLM-Norm (LayerNormalization)          (None, 50, 768)            1536          MLM-Dense[0][0]                         \n________________________________________________________________________________________________________________________\nExtract (Extract)                      (None, 768)                0             Encoder-12-FeedForward-Norm[0][0]       \n________________________________________________________________________________________________________________________\nMLM-Sim (EmbeddingSimilarity)          (None, 50, 30522)          30522         MLM-Norm[0][0]                          \n                                                                                Embedding-Token[0][1]                   \n________________________________________________________________________________________________________________________\nInput-Masked (InputLayer)              (None, 50)                 0                                                     \n________________________________________________________________________________________________________________________\nNSP-Dense (Dense)                      (None, 768)                590592        Extract[0][0]                           \n________________________________________________________________________________________________________________________\nMLM (Masked)                           (None, 50, 30522)          0             MLM-Sim[0][0]                           \n                                                                                Input-Masked[0][0]                      \n________________________________________________________________________________________________________________________\nNSP (Dense)                            (None, 2)                  1538          NSP-Dense[0][0]                         \n========================================================================================================================\nTotal params: 109,751,612\nTrainable params: 109,751,612\nNon-trainable params: 0\n________________________________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\nsave_path = 'distilbert_base_uncased/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)","execution_count":5,"outputs":[{"output_type":"stream","text":"I0825 10:15:35.362718 139826089715072 file_utils.py:39] PyTorch version 1.0.1.post2 available.\nI0825 10:15:37.059812 139826089715072 filelock.py:254] Lock 139822683457520 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\nI0825 10:15:37.062653 139826089715072 file_utils.py:748] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4y6es8jy\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d38421ed8284abf97fd8413b90da169"}},"metadata":{}},{"output_type":"stream","text":"I0825 10:15:37.818794 139826089715072 file_utils.py:752] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\nI0825 10:15:37.820354 139826089715072 file_utils.py:755] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\nI0825 10:15:37.823983 139826089715072 filelock.py:317] Lock 139822683457520 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\nI0825 10:15:37.824700 139826089715072 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"('distilbert_base_uncased/vocab.txt',\n 'distilbert_base_uncased/special_tokens_map.json',\n 'distilbert_base_uncased/added_tokens.json')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tokenizers import BertWordPieceTokenizer\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)\nfast_tokenizer","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense,Input,Flatten,concatenate,Dropout,Lambda\nfrom keras.models import Model\nimport keras.backend as K\nimport re\nimport codecs\n\nsequence_output  = model.layers[-6].output\npool_output = Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),name = 'real_output')(sequence_output)\nmodel3  = Model(inputs=model.input, outputs=pool_output)\nmodel3.compile(loss='binary_crossentropy', optimizer=adam)\nmodel3.summary()","execution_count":7,"outputs":[{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nInput-Token (InputLayer)        (None, 50)           0                                            \n__________________________________________________________________________________________________\nInput-Segment (InputLayer)      (None, 50)           0                                            \n__________________________________________________________________________________________________\nEmbedding-Token (TokenEmbedding [(None, 50, 768), (3 23440896    Input-Token[0][0]                \n__________________________________________________________________________________________________\nEmbedding-Segment (Embedding)   (None, 50, 768)      1536        Input-Segment[0][0]              \n__________________________________________________________________________________________________\nEmbedding-Token-Segment (Add)   (None, 50, 768)      0           Embedding-Token[0][0]            \n                                                                 Embedding-Segment[0][0]          \n__________________________________________________________________________________________________\nEmbedding-Position (PositionEmb (None, 50, 768)      38400       Embedding-Token-Segment[0][0]    \n__________________________________________________________________________________________________\nEmbedding-Dropout (Dropout)     (None, 50, 768)      0           Embedding-Position[0][0]         \n__________________________________________________________________________________________________\nEmbedding-Norm (LayerNormalizat (None, 50, 768)      1536        Embedding-Dropout[0][0]          \n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 50, 768)      2362368     Embedding-Norm[0][0]             \n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-1-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 50, 768)      0           Embedding-Norm[0][0]             \n                                                                 Encoder-1-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-1-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-1-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-1-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-1-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-1-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-1-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-1-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-1-MultiHeadSelfAttention-\n                                                                 Encoder-1-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-1-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-1-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-1-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-2-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-1-FeedForward-Norm[0][0] \n                                                                 Encoder-2-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-2-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-2-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-2-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-2-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-2-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-2-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-2-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-2-MultiHeadSelfAttention-\n                                                                 Encoder-2-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-2-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-2-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-2-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-3-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-2-FeedForward-Norm[0][0] \n                                                                 Encoder-3-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-3-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-3-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-3-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-3-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-3-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-3-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-3-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-3-MultiHeadSelfAttention-\n                                                                 Encoder-3-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-3-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-3-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-3-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-4-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-3-FeedForward-Norm[0][0] \n                                                                 Encoder-4-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-4-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-4-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-4-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-4-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-4-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-4-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-4-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-4-MultiHeadSelfAttention-\n                                                                 Encoder-4-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-4-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-4-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-4-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-5-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-4-FeedForward-Norm[0][0] \n                                                                 Encoder-5-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-5-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-5-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-5-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-5-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-5-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-5-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-5-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-5-MultiHeadSelfAttention-\n                                                                 Encoder-5-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-5-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-5-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-5-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-6-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-5-FeedForward-Norm[0][0] \n                                                                 Encoder-6-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-6-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-6-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-6-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-6-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-6-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-6-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-6-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-6-MultiHeadSelfAttention-\n                                                                 Encoder-6-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-6-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-6-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-6-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-7-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-6-FeedForward-Norm[0][0] \n                                                                 Encoder-7-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-7-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-7-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-7-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-7-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-7-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-7-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-7-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-7-MultiHeadSelfAttention-\n                                                                 Encoder-7-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-7-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-7-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-7-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-8-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-7-FeedForward-Norm[0][0] \n                                                                 Encoder-8-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-8-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-8-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-8-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-8-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-8-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-8-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-8-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-8-MultiHeadSelfAttention-\n                                                                 Encoder-8-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-8-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-8-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 50, 768)      2362368     Encoder-8-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-9-MultiHeadSelfAttention[\n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 50, 768)      0           Encoder-8-FeedForward-Norm[0][0] \n                                                                 Encoder-9-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-9-MultiHeadSelfAttentio (None, 50, 768)      1536        Encoder-9-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-9-FeedForward (FeedForw (None, 50, 768)      4722432     Encoder-9-MultiHeadSelfAttention-\n__________________________________________________________________________________________________\nEncoder-9-FeedForward-Dropout ( (None, 50, 768)      0           Encoder-9-FeedForward[0][0]      \n__________________________________________________________________________________________________\nEncoder-9-FeedForward-Add (Add) (None, 50, 768)      0           Encoder-9-MultiHeadSelfAttention-\n                                                                 Encoder-9-FeedForward-Dropout[0][\n__________________________________________________________________________________________________\nEncoder-9-FeedForward-Norm (Lay (None, 50, 768)      1536        Encoder-9-FeedForward-Add[0][0]  \n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 50, 768)      2362368     Encoder-9-FeedForward-Norm[0][0] \n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 50, 768)      0           Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 50, 768)      0           Encoder-9-FeedForward-Norm[0][0] \n                                                                 Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-MultiHeadSelfAttenti (None, 50, 768)      1536        Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-FeedForward (FeedFor (None, 50, 768)      4722432     Encoder-10-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-10-FeedForward-Dropout  (None, 50, 768)      0           Encoder-10-FeedForward[0][0]     \n__________________________________________________________________________________________________\nEncoder-10-FeedForward-Add (Add (None, 50, 768)      0           Encoder-10-MultiHeadSelfAttention\n                                                                 Encoder-10-FeedForward-Dropout[0]\n__________________________________________________________________________________________________\nEncoder-10-FeedForward-Norm (La (None, 50, 768)      1536        Encoder-10-FeedForward-Add[0][0] \n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 50, 768)      2362368     Encoder-10-FeedForward-Norm[0][0]\n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 50, 768)      0           Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 50, 768)      0           Encoder-10-FeedForward-Norm[0][0]\n                                                                 Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-MultiHeadSelfAttenti (None, 50, 768)      1536        Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-FeedForward (FeedFor (None, 50, 768)      4722432     Encoder-11-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-11-FeedForward-Dropout  (None, 50, 768)      0           Encoder-11-FeedForward[0][0]     \n__________________________________________________________________________________________________\nEncoder-11-FeedForward-Add (Add (None, 50, 768)      0           Encoder-11-MultiHeadSelfAttention\n                                                                 Encoder-11-FeedForward-Dropout[0]\n__________________________________________________________________________________________________\nEncoder-11-FeedForward-Norm (La (None, 50, 768)      1536        Encoder-11-FeedForward-Add[0][0] \n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 50, 768)      2362368     Encoder-11-FeedForward-Norm[0][0]\n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 50, 768)      0           Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 50, 768)      0           Encoder-11-FeedForward-Norm[0][0]\n                                                                 Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-MultiHeadSelfAttenti (None, 50, 768)      1536        Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-FeedForward (FeedFor (None, 50, 768)      4722432     Encoder-12-MultiHeadSelfAttention\n__________________________________________________________________________________________________\nEncoder-12-FeedForward-Dropout  (None, 50, 768)      0           Encoder-12-FeedForward[0][0]     \n__________________________________________________________________________________________________\nEncoder-12-FeedForward-Add (Add (None, 50, 768)      0           Encoder-12-MultiHeadSelfAttention\n                                                                 Encoder-12-FeedForward-Dropout[0]\n__________________________________________________________________________________________________\nEncoder-12-FeedForward-Norm (La (None, 50, 768)      1536        Encoder-12-FeedForward-Add[0][0] \n__________________________________________________________________________________________________\nExtract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n__________________________________________________________________________________________________\nreal_output (Dense)             (None, 1)            769         Extract[0][0]                    \n==================================================================================================\nTotal params: 108,537,601\nTrainable params: 108,537,601\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3.load_weights('../input/bert-weights-tweets/bert_weights_tweet.h5')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install tweepy","execution_count":9,"outputs":[{"output_type":"stream","text":"Collecting tweepy\n  Downloading https://files.pythonhosted.org/packages/bb/7c/99d51f80f3b77b107ebae2634108717362c059a41384a1810d13e2429a81/tweepy-3.9.0-py2.py3-none-any.whl\nCollecting requests-oauthlib>=0.7.0 (from tweepy)\n  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\nRequirement already satisfied: requests[socks]>=2.11.1 in /opt/conda/lib/python3.6/site-packages (from tweepy) (2.21.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tweepy) (1.12.0)\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->tweepy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n\u001b[K    100% |████████████████████████████████| 153kB 1.2MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (1.22)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (2.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (2019.3.9)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /opt/conda/lib/python3.6/site-packages (from requests[socks]>=2.11.1->tweepy) (1.6.8)\nInstalling collected packages: oauthlib, requests-oauthlib, tweepy\nSuccessfully installed oauthlib-3.1.0 requests-oauthlib-1.3.0 tweepy-3.9.0\n\u001b[33mYou are using pip version 19.0.3, however version 20.2.2 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re \nimport tweepy \nfrom tweepy import OAuthHandler \nfrom cleantext import clean","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  \nclass TwitterClient(object): \n    \n    def __init__(self): \n        \n        # keys and tokens from the Twitter Dev Console \n        consumer_key = ''\n        consumer_secret = ''\n        access_token = ''\n        access_token_secret = ''\n  \n        # attempt authentication \n        try: \n            # create OAuthHandler object \n            self.auth = OAuthHandler(consumer_key, consumer_secret) \n            # set access token and secret \n            self.auth.set_access_token(access_token, access_token_secret) \n            # create tweepy API object to fetch tweets \n            self.api = tweepy.API(self.auth) \n        except: \n            print(\"Error: Authentication Failed\") \n  \n    def clean_tweet(self, tweet): \n        \n        tweet=clean(tweet,\n        fix_unicode=True,               # fix various unicode errors\n        to_ascii=True,                  # transliterate to closest ASCII representation\n        lower=True,                     # lowercase text\n        no_line_breaks=True,           # fully strip line breaks as opposed to only normalizing them\n        no_urls=True,                  # replace all URLs with a special token\n        no_emails=True,                # replace all email addresses with a special token\n        no_phone_numbers=True,         # replace all phone numbers with a special token\n        no_numbers=True,               # replace all numbers with a special token\n        no_digits=True,                # replace all digits with a special token\n        no_currency_symbols=True,      # replace all currency symbols with a special token\n        no_punct=True,                 # fully remove punctuation\n        replace_with_url=\"<URL>\",\n        replace_with_email=\"<EMAIL>\",\n        replace_with_phone_number=\"<PHONE>\",\n        replace_with_number=\"<NUMBER>\",\n        replace_with_digit=\"0\",\n        replace_with_currency_symbol=\"<CUR>\",\n        lang=\"en\"                       # set to 'de' for German special handling\n        )\n        return tweet\n        \n    def convert_lines(self,tweet, max_seq_length,tokenizer):\n        max_seq_length -=2\n        all_tokens = []\n\n        tokens_a = tokenizer.tokenize(tweet)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n\n        return np.array(all_tokens)\n    \n    def get_tweet_sentiment(self, tweet): \n        \n        tweet2 = self.clean_tweet(tweet)\n        \n        token_input2 = self.convert_lines(tweet2,maxlen,tokenizer)\n        \n        seg_input2 = np.zeros((token_input2.shape[0],maxlen))\n        mask_input2 = np.ones((token_input2.shape[0],maxlen))\n        \n        hehe = model3.predict([token_input2, seg_input2, mask_input2],verbose=1,batch_size=32)\n        \n        if hehe <= 0.5: \n            return 'no disaster'\n        else: \n            return 'real disaster'\n        \n  \n    def get_tweets(self, query, count = 10): \n        ''' \n        Main function to fetch tweets and parse them. \n        '''\n        # empty list to store parsed tweets \n        tweets = [] \n  \n        try: \n            # call twitter api to fetch tweets \n            fetched_tweets = self.api.search(q = query, count = count) \n  \n            # parsing tweets one by one \n            for tweet in fetched_tweets: \n                # empty dictionary to store required params of a tweet \n                parsed_tweet = {} \n  \n                # saving text of tweet \n                parsed_tweet['text'] = tweet.text \n                # saving sentiment of tweet \n                parsed_tweet['class'] = self.get_tweet_sentiment(tweet.text) \n  \n                # appending parsed tweet to tweets list \n                if tweet.retweet_count > 0: \n                    # if tweet has retweets, ensure that it is appended only once \n                    if parsed_tweet not in tweets: \n                        tweets.append(parsed_tweet) \n                else: \n                    tweets.append(parsed_tweet) \n  \n            # return parsed tweets \n            return tweets \n  \n        except tweepy.TweepError as e: \n            # print error (if any) \n            print(\"Error : \" + str(e)) \n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"api = TwitterClient() \n \ntweets = api.get_tweets(query = 'crime', count = 200) \n\n\nptweets = [tweet for tweet in tweets if tweet['class'] == 'real disaster']  \nprint(\"Real Disaster tweets percentage: {} %\".format(100*len(ptweets)/len(tweets))) \n\nntweets = [tweet for tweet in tweets if tweet['class'] == 'no disaster'] \nprint(\"No Disaster tweets percentage: {} %\".format(100*len(ntweets)/len(tweets))) \n\n\n# printing first 5 positive tweets \nprint(\"\\n\\n Real Disaster tweets:\") \nfor tweet in ptweets[:10]: \n    print(tweet['text']) \n\n# printing first 5 negative tweets \nprint(\"\\n\\n No Disaster tweets:\") \nfor tweet in ntweets[:10]: \n    print(tweet['text']) \n","execution_count":16,"outputs":[{"output_type":"stream","text":"1/1 [==============================] - 0s 152ms/step\n1/1 [==============================] - 0s 162ms/step\n1/1 [==============================] - 0s 155ms/step\n1/1 [==============================] - 0s 145ms/step\n1/1 [==============================] - 0s 155ms/step\n1/1 [==============================] - 0s 152ms/step\n1/1 [==============================] - 0s 147ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 147ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 192ms/step\n1/1 [==============================] - 0s 160ms/step\n1/1 [==============================] - 0s 163ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 147ms/step\n1/1 [==============================] - 0s 157ms/step\n1/1 [==============================] - 0s 169ms/step\n1/1 [==============================] - 0s 169ms/step\n1/1 [==============================] - 0s 163ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 174ms/step\n1/1 [==============================] - 0s 168ms/step\n1/1 [==============================] - 0s 168ms/step\n1/1 [==============================] - 0s 160ms/step\n1/1 [==============================] - 0s 154ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 152ms/step\n1/1 [==============================] - 0s 152ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 162ms/step\n1/1 [==============================] - 0s 194ms/step\n1/1 [==============================] - 0s 173ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 161ms/step\n1/1 [==============================] - 0s 176ms/step\n1/1 [==============================] - 0s 168ms/step\n1/1 [==============================] - 0s 155ms/step\n1/1 [==============================] - 0s 158ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 154ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 147ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 157ms/step\n1/1 [==============================] - 0s 154ms/step\n1/1 [==============================] - 0s 145ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 147ms/step\n1/1 [==============================] - 0s 157ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 147ms/step\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 152ms/step\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 152ms/step\n1/1 [==============================] - 0s 163ms/step\n1/1 [==============================] - 0s 206ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 161ms/step\n1/1 [==============================] - 0s 150ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 147ms/step\n1/1 [==============================] - 0s 151ms/step\n1/1 [==============================] - 0s 196ms/step\n1/1 [==============================] - 0s 196ms/step\n1/1 [==============================] - 0s 187ms/step\n1/1 [==============================] - 0s 194ms/step\n1/1 [==============================] - 0s 160ms/step\n1/1 [==============================] - 0s 149ms/step\n1/1 [==============================] - 0s 152ms/step\n1/1 [==============================] - 0s 166ms/step\n1/1 [==============================] - 0s 163ms/step\n1/1 [==============================] - 0s 159ms/step\n1/1 [==============================] - 0s 153ms/step\n1/1 [==============================] - 0s 151ms/step\nReal Disaster tweets percentage: 28.35820895522388 %\nNo Disaster tweets percentage: 71.64179104477611 %\n\n\n Real Disaster tweets:\nRT @Kamo_Palate: Lol I’m realizing a lot of people don’t think rape is an actual crime. They think it’s just a misunderstanding. An oops, l…\nRT @MrAndyNgo: New footage shows Jacob Blake brawling with cops before being shot. The #Kenosha man, who survived, has a criminal history t…\nRT @UnSubtleDesi: They called her ‘kafir’: Hindu woman Santola Devi was lynched to death by 7 Muslims in UP, her family alleges police apat…\nRT @SherniIsBack07: Rhea's Connection with Drug Dealing???? \nAnother Crime....!! \nBut this can't be happen without any Big Name's support..…\nRT @Alokmishra416: Big Breaking !!\n\nLog soch rhe the Dal m kuch kala h.\n\nYaha to poori Daal hi kaali h.\n\nSare ke sare Police officials jo 1…\n@mawson_craig @corneliusdelro @KarlyRican Your argument is that genocide is a political structure &amp; crime as outlin… https://t.co/3gwBIS5DMi\nRT @akshay14793: Its a crime that SPB didn't win a national award for Mun Paniya\nRT @Jim_Jordan: This President has shared private moments like this with families of soldiers, victims of violent crime, and people who’ve…\n@johansakib__1D yeah but he still attempted to rape someone. muslim or not it's still a CRIME .\nRT @Thomas1774Paine: Minneapolis Residents Sue City Over Spike In Crime, Demand City Stop Campaign To Defund Police https://t.co/VKGsQyhx4r\n\n\n No Disaster tweets:\nRT @wildforest_matt: I have a commission for a feature on Rural crime, particularly coursing. I'm looking to meet those on the frontline -…\nRT @OutProud4: The ‘crime’ of sodomy criminalises same-sex acts in #eSwatini. Though not applied in recent history, the law is used to deny…\nRT @iAnujShukla: उत्तर प्रदेश में अपराध चरम पर है। हर दिन राज्य के किसी न किसी हिस्से से रेप, हत्या, लूट जैसी घटनाओं की खबरें सामने आती रह…\nRT @WALLOWSREGRETS: PEDOPHILIA IS NOT A MISTAKE IT IS A CRIME.\nRT @Limportant_fr: Etre soi-même est un crime en République islamique d'Iran. Nous ne luttons pas contre un petit morceau de tissu, nous lu…\n@PierreAtlas @BillKristol What do you think \"Make America Great Again\" means? The 50's were a great time for the Tr… https://t.co/Sls86uo2F4\nRT @bueti: Hong Kong court rejects challenge to national security law’s new bail rules. As was to be expected. Because Beijing mandated: Na…\nRT @SylvieGelinet: 📌FORCED VAXX is a CRIME AGAINST HUMANITY. Those who are not aware of it yet must read and have a copy with them .... Res…\nA crime worse than killing in my opinion. The Super Bowl’s Biggest Losers: The Boys and Girls Being Sold for Sex 20… https://t.co/cjBM7fn1B2\nRT @ggreenwald: One of the DNC speakers was @DonnaHylton, who, at 21, committed an unspeakably gruesome crime: over 15 days she was part of…\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}